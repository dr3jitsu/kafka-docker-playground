---
version: '3.5'
services:

  producer-repro-98764:
    build:
      context: ../../reproduction-models/connect-connect-hdfs2-sink/producer-repro-98764/
    hostname: producer
    container_name: producer-repro-98764
    environment:
      KAFKA_BOOTSTRAP_SERVERS: broker:9092
      TOPIC: "customer_avro"
      REPLICATION_FACTOR: 1
      NUMBER_OF_PARTITIONS: 1
      NB_MESSAGES: 10 # -1 for MAX_VALUE
      MESSAGE_BACKOFF: 1000 # Frequency of message injection
      KAFKA_ACKS: "all" # default: "1"
      KAFKA_REQUEST_TIMEOUT_MS: 20000
      KAFKA_RETRY_BACKOFF_MS: 500
      KAFKA_CLIENT_ID: "my-java-producer-repro-98764"
      KAFKA_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
      JAVA_OPTS: 
    volumes:
      - ../../environment/plaintext/jmx-exporter:/usr/share/jmx_exporter/

  producer-repro-98764-2:
    build:
      context: ../../reproduction-models/connect-connect-hdfs2-sink/producer-repro-98764-2/
    hostname: producer
    container_name: producer-repro-98764-2
    environment:
      KAFKA_BOOTSTRAP_SERVERS: broker:9092
      TOPIC: "customer_avro"
      REPLICATION_FACTOR: 1
      NUMBER_OF_PARTITIONS: 1
      NB_MESSAGES: 10 # -1 for MAX_VALUE
      MESSAGE_BACKOFF: 1000 # Frequency of message injection
      KAFKA_ACKS: "all" # default: "1"
      KAFKA_REQUEST_TIMEOUT_MS: 20000
      KAFKA_RETRY_BACKOFF_MS: 500
      KAFKA_CLIENT_ID: "my-java-producer-repro-98764-2"
      KAFKA_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
      JAVA_OPTS: 
    volumes:
      - ../../environment/plaintext/jmx-exporter:/usr/share/jmx_exporter/



  # using https://github.com/big-data-europe/docker-hive

  namenode:
    hostname: namenode
    container_name: namenode
    image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8
    volumes:
      - namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ../../connect/connect-hdfs2-sink/hadoop-hive.env
    ports:
      - "50070:50070"

  datanode:
    hostname: datanode
    container_name: datanode
    image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8
    volumes:
      - datanode:/hadoop/dfs/data
    env_file:
      - ../../connect/connect-hdfs2-sink/hadoop-hive.env
    environment:
      SERVICE_PRECONDITION: "namenode:50070"
    ports:
      - "50075:50075"

  hive-server:
    hostname: hive-server
    container_name: hive-server
    image: bde2020/hive:2.3.2-postgresql-metastore
    env_file:
      - ../../connect/connect-hdfs2-sink/hadoop-hive.env
    environment:
      HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-metastore/metastore"
      SERVICE_PRECONDITION: "hive-metastore:9083"

  hive-metastore:
    hostname: hive-metastore
    container_name: hive-metastore
    image: bde2020/hive:2.3.2-postgresql-metastore
    env_file:
      - ../../connect/connect-hdfs2-sink/hadoop-hive.env
    command: /opt/hive/bin/hive --service metastore
    environment:
      SERVICE_PRECONDITION: "namenode:50070 datanode:50075 hive-metastore-postgresql:5432"
    ports:
      - "9083:9083"

  hive-metastore-postgresql:
    hostname: hive-metastore-postgresql
    container_name: hive-metastore-postgresql
    image: bde2020/hive-metastore-postgresql:2.3.0

  presto-coordinator:
    hostname: presto-coordinator
    container_name: presto-coordinator
    image: shawnzhu/prestodb:0.181
    ports:
      - "18080:8080"

  connect:
    depends_on:
      - zookeeper
      - broker
      - schema-registry
      - hive-server
      - presto-coordinator
      - hive-metastore
    environment:
      CONNECT_PLUGIN_PATH: /usr/share/confluent-hub-components/confluentinc-kafka-connect-hdfs

volumes:
  namenode:
  datanode:

networks:
  common-network:
    driver: overlay